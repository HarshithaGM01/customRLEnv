{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWLHwDxjUYOFhw0mgMnNWr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshithaGM01/customRLEnv/blob/main/CustomRLEnv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d39x-u7b3Tdi",
        "outputId": "0ebaaca1-af10-49a4-a815-59de8ef2a4cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12\n",
            "Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n"
          ]
        }
      ],
      "source": [
        "import sys, platform, random\n",
        "import numpy as np\n",
        "\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"Platform:\", platform.platform())\n",
        "\n",
        "SEED_271 = 271\n",
        "random.seed(SEED_271)\n",
        "np.random.seed(SEED_271)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install \"gymnasium>=0.29\" \"stable-baselines3>=2.2.1\" \"shimmy>=1.3.0\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j-eRdNC36rG",
        "outputId": "240ac0e2-06c2-4c27-bfa8-a5c05ce3f276"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/188.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m184.3/188.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.0/188.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "OxFRrd3w4JYw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LongHorizonDecisionEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": []}\n",
        "    def __init__(self, max_steps=200, seed=271):\n",
        "      super().__init__()\n",
        "\n",
        "      self.max_steps = max_steps\n",
        "      self.current_step = 0\n",
        "\n",
        "      self._rng = np.random.default_rng(seed)\n",
        "\n",
        "      #observation space - we have [skill,energy,resources,confidence,market_noise]\n",
        "      self.observation_space = spaces.Box(\n",
        "          low=0.0,\n",
        "          high=1.0,\n",
        "          shape=(5,),\n",
        "          dtype=np.float32\n",
        "      )\n",
        "\n",
        "      #Action space - 0: invest in a skill\n",
        "      # 1: exploit current skill\n",
        "      # 2: rest/recover\n",
        "      # 3: take risky opportunity\n",
        "      # 4: play safe\n",
        "      self.action_space = spaces.Discrete(5)\n",
        "\n",
        "      self.state = None\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "      super().reset(seed=seed)\n",
        "\n",
        "      if seed is not None:\n",
        "        self._rng = np.random.default_rng(seed)\n",
        "\n",
        "      self.current_step = 0\n",
        "\n",
        "      skill = self._rng.uniform(0.1, 0.3)\n",
        "      energy = self._rng.uniform(0.6, 0.9)\n",
        "      resources = self._rng.uniform(0.4, 0.7)\n",
        "      confidence = self._rng.uniform(0.3, 0.6)\n",
        "      market_noise = self._rng.uniform(0.0, 1.0)\n",
        "\n",
        "      self.state = np.array(\n",
        "            [skill, energy, resources, confidence, market_noise],\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "      return self.state, {}\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "      skill, energy, resources, confidence, market_noise = self.state.astype(np.float32)\n",
        "\n",
        "      # --- dynamics knobs (tunable later) ---\n",
        "      invest_cost = 0.04\n",
        "      exploit_cost_energy = 0.05\n",
        "      exploit_gain_res = 0.06\n",
        "\n",
        "      rest_gain_energy = 0.10\n",
        "      rest_cost_res = 0.02\n",
        "\n",
        "      safe_gain_res = 0.03\n",
        "      safe_cost_energy = 0.01\n",
        "\n",
        "      risky_cost_energy = 0.06\n",
        "      risky_cost_res = 0.03\n",
        "      risky_success_base = 0.20  # base success prob\n",
        "      risky_success_skill_boost = 0.60  # extra prob from skill\n",
        "\n",
        "      # Market noise updates each step (stochastic world)\n",
        "      market_noise = float(self._rng.uniform(0.0, 1.0))\n",
        "\n",
        "      # Track reward components for analysis\n",
        "      immediate_reward = 0.0\n",
        "\n",
        "      # --- apply action ---\n",
        "      if action == 0:\n",
        "        # invest in skill: slow payoff, upfront cost\n",
        "        resources -= invest_cost\n",
        "        energy -= 0.02\n",
        "        skill += 0.03 + 0.01 * confidence\n",
        "        confidence += 0.01\n",
        "        immediate_reward -= 0.01  # tiny immediate penalty (delayed reward theme)\n",
        "\n",
        "      elif action == 1:\n",
        "        # exploit current skill: immediate gain but drains energy\n",
        "        energy -= exploit_cost_energy\n",
        "        gain = exploit_gain_res * (0.5 + skill) * (0.7 + 0.3 * market_noise)\n",
        "        resources += gain\n",
        "        confidence += 0.01\n",
        "        immediate_reward += 0.5 * gain  # partial immediate reward\n",
        "\n",
        "      elif action == 2:\n",
        "        # rest: recovers energy, small cost to resources (time passes)\n",
        "        energy += rest_gain_energy\n",
        "        resources -= rest_cost_res\n",
        "        confidence += 0.005\n",
        "        immediate_reward -= 0.005\n",
        "\n",
        "      elif action == 3:\n",
        "        # risky opportunity: success depends on skill + randomness\n",
        "        energy -= risky_cost_energy\n",
        "        resources -= risky_cost_res\n",
        "\n",
        "        p_success = risky_success_base + risky_success_skill_boost * skill\n",
        "        p_success = float(np.clip(p_success, 0.0, 0.95))\n",
        "        success = (self._rng.random() < p_success)\n",
        "\n",
        "        if success:\n",
        "          big_gain = 0.20 + 0.25 * market_noise + 0.10 * confidence\n",
        "          resources += big_gain\n",
        "          confidence += 0.04\n",
        "          immediate_reward += big_gain\n",
        "        else:\n",
        "          confidence -= 0.03\n",
        "          immediate_reward -= 0.03\n",
        "\n",
        "      elif action == 4:\n",
        "        # play safe: small reliable gain, minimal drain\n",
        "        resources += safe_gain_res * (0.8 + 0.2 * market_noise)\n",
        "        energy -= safe_cost_energy\n",
        "        confidence += 0.002\n",
        "        immediate_reward += 0.01\n",
        "\n",
        "      else:\n",
        "        raise ValueError(f\"Invalid action: {action}\")\n",
        "\n",
        "      # --- natural dynamics / constraints ---\n",
        "      # burnout effect: if energy too low, confidence and resources suffer\n",
        "      if energy < 0.15:\n",
        "        immediate_reward -= 0.05\n",
        "        confidence -= 0.02\n",
        "        resources -= 0.02\n",
        "\n",
        "      # clamp to [0,1]\n",
        "      skill = float(np.clip(skill, 0.0, 1.0))\n",
        "      energy = float(np.clip(energy, 0.0, 1.0))\n",
        "      resources = float(np.clip(resources, 0.0, 1.0))\n",
        "      confidence = float(np.clip(confidence, 0.0, 1.0))\n",
        "\n",
        "      self.state = np.array([skill, energy, resources, confidence, market_noise], dtype=np.float32)\n",
        "\n",
        "      # --- delayed reward component ---\n",
        "      # We want long-term stability + growth:\n",
        "      # - reward increases with resources and skill\n",
        "      # - penalty for low energy (burnout) and stagnation\n",
        "      burnout_penalty = max(0.0, 0.25 - energy)  # only penalize when energy is low\n",
        "      growth_reward = 0.6 * resources + 0.4 * skill\n",
        "      reward = float(growth_reward - 0.8 * burnout_penalty + 0.2 * immediate_reward)\n",
        "\n",
        "      self.current_step += 1\n",
        "\n",
        "      terminated = False\n",
        "      truncated = False\n",
        "\n",
        "      # terminate if resources depleted badly (collapse) or extreme burnout\n",
        "      if resources <= 0.02 or energy <= 0.02:\n",
        "        terminated = True\n",
        "\n",
        "      # truncate if time horizon reached\n",
        "      if self.current_step >= self.max_steps:\n",
        "        truncated = True\n",
        "\n",
        "      info = {\n",
        "        \"skill\": skill,\n",
        "        \"energy\": energy,\n",
        "        \"resources\": resources,\n",
        "        \"confidence\": confidence,\n",
        "        \"market_noise\": market_noise,\n",
        "        \"burnout_penalty\": burnout_penalty,\n",
        "        \"immediate_reward\": immediate_reward,\n",
        "        \"growth_reward\": growth_reward\n",
        "      }\n",
        "\n",
        "      return self.state, reward, terminated, truncated, info\n",
        "\n"
      ],
      "metadata": {
        "id": "B6uKlKyg4fhQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create env + run a few random steps to sanity check\n",
        "env_271 = LongHorizonDecisionEnv(max_steps=20, seed=271)\n",
        "\n",
        "obs, info = env_271.reset()\n",
        "print(\"Reset obs:\", obs)\n",
        "\n",
        "for t in range(5):\n",
        "    a = env_271.action_space.sample()\n",
        "    obs, reward, terminated, truncated, info = env_271.step(a)\n",
        "    print(f\"\\nStep {t+1}\")\n",
        "    print(\"  action:\", a)\n",
        "    print(\"  obs:\", obs)\n",
        "    print(\"  reward:\", reward)\n",
        "    print(\"  terminated:\", terminated, \"| truncated:\", truncated)\n",
        "    print(\"  info (key fields):\",\n",
        "          {k: round(info[k], 3) for k in [\"skill\",\"energy\",\"resources\",\"confidence\",\"burnout_penalty\"]})\n",
        "    if terminated or truncated:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4UtbiQWTqtW",
        "outputId": "96d458e4-efe6-433c-b02e-a2a2a14bbeb4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reset obs: [0.2563959  0.8538769  0.5808103  0.3747876  0.19264777]\n",
            "\n",
            "Step 1\n",
            "  action: 4\n",
            "  obs: [0.2563959 0.8438769 0.6078727 0.3767876 0.5104025]\n",
            "  reward: 0.4692819972038269\n",
            "  terminated: False | truncated: False\n",
            "  info (key fields): {'skill': 0.256, 'energy': 0.844, 'resources': 0.608, 'confidence': 0.377, 'burnout_penalty': 0.0}\n",
            "\n",
            "Step 2\n",
            "  action: 1\n",
            "  obs: [0.2563959  0.7938769  0.6438893  0.3867876  0.31200263]\n",
            "  reward: 0.4924936294555664\n",
            "  terminated: False | truncated: False\n",
            "  info (key fields): {'skill': 0.256, 'energy': 0.794, 'resources': 0.644, 'confidence': 0.387, 'burnout_penalty': 0.0}\n",
            "\n",
            "Step 3\n",
            "  action: 3\n",
            "  obs: [0.2563959  0.7338769  0.922181   0.42678759 0.27845174]\n",
            "  reward: 0.7175253033638\n",
            "  terminated: False | truncated: False\n",
            "  info (key fields): {'skill': 0.256, 'energy': 0.734, 'resources': 0.922, 'confidence': 0.427, 'burnout_penalty': 0.0}\n",
            "\n",
            "Step 4\n",
            "  action: 1\n",
            "  obs: [0.2563959  0.6838769  0.9609017  0.43678758 0.51061034]\n",
            "  reward: 0.6829714179039001\n",
            "  terminated: False | truncated: False\n",
            "  info (key fields): {'skill': 0.256, 'energy': 0.684, 'resources': 0.961, 'confidence': 0.437, 'burnout_penalty': 0.0}\n",
            "\n",
            "Step 5\n",
            "  action: 2\n",
            "  obs: [0.2563959  0.7838769  0.9409017  0.44178757 0.6707056 ]\n",
            "  reward: 0.6660993804931641\n",
            "  terminated: False | truncated: False\n",
            "  info (key fields): {'skill': 0.256, 'energy': 0.784, 'resources': 0.941, 'confidence': 0.442, 'burnout_penalty': 0.0}\n"
          ]
        }
      ]
    }
  ]
}